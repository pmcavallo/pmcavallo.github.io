# ğŸ“‰ Customer Churn Prediction App (Deployed on Render)

This project is an end-to-end machine learning web app for predicting customer churn using a trained classification model.

ğŸ”— **Live App:** [https://churn-prediction-app-dxft.onrender.com](https://churn-prediction-app-dxft.onrender.com)

---

## ğŸ“¸ App Preview

![Churn Prediction App Screenshot](screenshots/streamlit_ui.png)

---

## ğŸ›  Project Overview

This app:
- Trains a `RandomForestClassifier` to predict churn
- Encodes/preprocesses input features using `ColumnTransformer`
- Uses `joblib` to save/load model artifacts
- Provides a user-friendly interface using `Streamlit`
- Is deployed serverlessly using **Render**

---

## ğŸ“ Project Structure

```
churn-prediction-app/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                  # Streamlit web interface
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ train_model.py          # Training logic
â”‚   â”œâ”€â”€ preprocess.py           # Data loading/preprocessing
â”‚   â”œâ”€â”€ churn_model.pkl         # Trained classifier
â”‚   â””â”€â”€ preprocessor.pkl        # Encoded transformer
â”œâ”€â”€ data/
â”‚   â””â”€â”€ telco_churn.csv         # Simulated dataset
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ render.yaml                 # Render deployment config
â”œâ”€â”€ screenshots/
â”‚   â””â”€â”€ streamlit_ui.png        # App screenshot
â””â”€â”€ README.md                   # Project documentation
```

---

## âš™ï¸ Local Setup

### âœ… 1. Clone the Repo

```bash
git clone https://github.com/YOUR_USERNAME/churn-prediction-app.git
cd churn-prediction-app
```

### âœ… 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### âœ… 3. Train the Model (optional if .pkl files exist)

```bash
python model/train_model.py
```

This saves:
- `model/churn_model.pkl`
- `model/preprocessor.pkl`

### âœ… 4. Launch the Streamlit App

```bash
streamlit run app/app.py
```

---

## ğŸŒ Deploying to Render

Render is a free serverless platform that supports Python + Streamlit.

### âœ… Setup Steps

1. Push all files to a GitHub repo
2. Go to [https://render.com](https://render.com)
3. Click **"New Web Service"**
4. Connect your GitHub repo
5. Configure the following:
   - **Environment**: `Python`
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `streamlit run app/app.py --server.port $PORT`
6. Done! ğŸ‰ Your app is live.

ğŸ“Œ Add a `render.yaml` like this to automate config:

```yaml
services:
  - type: web
    name: churn-prediction-app
    env: python
    plan: free
    buildCommand: pip install -r requirements.txt
    startCommand: streamlit run app/app.py --server.port $PORT
    autoDeploy: true
```

---

## ğŸ¤– Tech Stack

| Purpose         | Tool            |
|----------------|-----------------|
| Language        | Python 3        |
| ML Library      | scikit-learn    |
| Web UI          | Streamlit       |
| Deployment      | Render          |
| Model Storage   | joblib          |
| Dataset         | Simulated Telco Churn |

---

## ğŸ“¬ Author

**Paulo Cavallo**  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/paulo-cavallo/)  
ğŸ§  [GitHub](https://github.com/YOUR_USERNAME)

---

## ğŸ“„ License

This project is available under the MIT License.
---

## ğŸ§ª Model Training and Preprocessing (Python)

### 1. Load and Preprocess Data

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

def load_data(path):
    df = pd.read_csv(path)
    return df

def preprocess_and_split(df):
    X = df.drop('Churn', axis=1)
    y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

    categorical = X.select_dtypes(include='object').columns.tolist()
    numerical = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

    preprocessor = ColumnTransformer(transformers=[
        ('num', StandardScaler(), numerical),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical)
    ])

    X_transformed = preprocessor.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
    return X_train, X_test, y_train, y_test, preprocessor
```

---

### 2. Train and Save the Model

```python
from sklearn.ensemble import RandomForestClassifier
import joblib

def train_and_save_model(X_train, y_train, preprocessor):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    joblib.dump(model, 'model/churn_model.pkl')
    joblib.dump(preprocessor, 'model/preprocessor.pkl')

    return model
```

---

### 3. Execute Training Script

```python
if __name__ == "__main__":
    df = load_data("data/telco_churn.csv")
    X_train, X_test, y_train, y_test, preprocessor = preprocess_and_split(df)
    train_and_save_model(X_train, y_train, preprocessor)
```

---

## ğŸ§¾ Notes

- We use `ColumnTransformer` to preprocess numerical and categorical features.
- The target `Churn` is binary encoded.
- Final model and preprocessor are saved as `.pkl` files for use in the web app.

